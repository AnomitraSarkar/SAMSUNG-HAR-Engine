{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clapping', 'Running']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "dataset_path = os.listdir('dataset/train')\n",
    "\n",
    "label_types = os.listdir('dataset/train')\n",
    "print (label_types)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        tag                               video_name\n",
      "0  Clapping  dataset/train/Clapping/Clapping_500.mp4\n",
      "1   Running    dataset/train/Running/Running_500.mp4\n",
      "        tag                               video_name\n",
      "0  Clapping  dataset/train/Clapping/Clapping_500.mp4\n",
      "1   Running    dataset/train/Running/Running_500.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rooms = []\n",
    "\n",
    "for item in dataset_path:\n",
    " # Get all the file names\n",
    " all_rooms = os.listdir('dataset/train' + '/' +item)\n",
    "\n",
    " # Add them to the list\n",
    " for room in all_rooms:\n",
    "    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))\n",
    "    \n",
    "# Build a dataframe        \n",
    "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
    "print(train_df.head())\n",
    "print(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.loc[:,['video_name','tag']]\n",
    "df\n",
    "df.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clapping', 'Running']\n",
      "Types of activities found:  2\n",
      "        tag                                 video_name\n",
      "0  Clapping     dataset/test/Clapping/Clapping_500.mp4\n",
      "1   Running  dataset/test/Running/Running_500 copy.mp4\n",
      "        tag                                 video_name\n",
      "0  Clapping     dataset/test/Clapping/Clapping_500.mp4\n",
      "1   Running  dataset/test/Running/Running_500 copy.mp4\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.listdir('dataset/test')\n",
    "print(dataset_path)\n",
    "\n",
    "room_types = os.listdir('dataset/test')\n",
    "print(\"Types of activities found: \", len(dataset_path))\n",
    "\n",
    "rooms = []\n",
    "\n",
    "for item in dataset_path:\n",
    " # Get all the file names\n",
    " all_rooms = os.listdir('dataset/test' + '/' +item)\n",
    "\n",
    " # Add them to the list\n",
    " for room in all_rooms:\n",
    "    rooms.append((item, str('dataset/test' + '/' +item) + '/' + room))\n",
    "    \n",
    "# Build a dataframe        \n",
    "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
    "print(test_df.head())\n",
    "print(test_df.tail())\n",
    "\n",
    "df = test_df.loc[:,['video_name','tag']]\n",
    "df\n",
    "df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tensorflow/docs\n",
      "  Cloning https://github.com/tensorflow/docs to c:\\users\\anomi\\appdata\\local\\temp\\pip-req-build-9eon7vxd\n",
      "  Resolved https://github.com/tensorflow/docs to commit c221d1e1af1ef5cc37c4a0879876f25f9cc1d981\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: astor in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-docs==2024.11.18.43811) (0.8.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-docs==2024.11.18.43811) (2.1.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-docs==2024.11.18.43811) (3.1.2)\n",
      "Requirement already satisfied: nbformat in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-docs==2024.11.18.43811) (5.10.4)\n",
      "Requirement already satisfied: protobuf>=3.12 in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-docs==2024.11.18.43811) (4.25.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-docs==2024.11.18.43811) (6.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->tensorflow-docs==2024.11.18.43811) (2.1.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat->tensorflow-docs==2024.11.18.43811) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat->tensorflow-docs==2024.11.18.43811) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\anomi\\appdata\\roaming\\python\\python312\\site-packages (from nbformat->tensorflow-docs==2024.11.18.43811) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat->tensorflow-docs==2024.11.18.43811) (5.13.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.11.18.43811) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.11.18.43811) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.11.18.43811) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.11.18.43811) (0.22.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\anomi\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->tensorflow-docs==2024.11.18.43811) (4.2.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\anomi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->tensorflow-docs==2024.11.18.43811) (306)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/docs 'C:\\Users\\anomi\\AppData\\Local\\Temp\\pip-req-build-9eon7vxd'\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 2\n",
      "Total videos for testing: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dataset/train/Running/Running_500.mp4</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                             video_name      tag\n",
       "1           1  dataset/train/Running/Running_500.mp4  Running"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two methods are taken from this tutorial:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "IMG_SIZE = 224\n",
    "\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 2us/step\n"
     ]
    }
   ],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clapping', 'Running']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
    "print(label_processor.get_vocabulary())\n",
    "\n",
    "labels = train_df[\"tag\"].values\n",
    "labels = label_processor(labels[..., None]).numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_data[0].shape)\n",
    "#train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "        frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "        mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "        # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "        # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "        # Use the CPU implementation of GRU by setting recurrent_activation='sigmoid'\n",
    "        # and reset_after=True if you encounter CuDNN errors.\n",
    "        x = keras.layers.GRU(16, return_sequences=True, recurrent_activation='sigmoid', reset_after=False)(\n",
    "            frame_features_input, mask=mask_input\n",
    "        )\n",
    "        x = keras.layers.GRU(8, recurrent_activation='sigmoid', reset_after=True)(x)\n",
    "        x = keras.layers.Dropout(0.4)(x)\n",
    "        x = keras.layers.Dense(224, activation=\"leaky_relu\")(x)\n",
    "        x = keras.layers.Dense(56, activation=\"leaky_relu\")(x)\n",
    "        x = keras.layers.Dense(14, activation=\"leaky_relu\")(x)\n",
    "        x = keras.layers.Dense(8, activation=\"leaky_relu\")(x)\n",
    "        output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def create_cnn_rnn_model(MAX_SEQ_LENGTH, NUM_FEATURES, class_vocab):\n",
    "    with tf.device('/CPU:0'):\n",
    "        # Input layers\n",
    "        frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "        mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "        # Reshape input for convolution (adding a channel dimension)\n",
    "        reshaped_input = keras.layers.Reshape((MAX_SEQ_LENGTH, NUM_FEATURES, 1))(frame_features_input)\n",
    "\n",
    "        # Convolutional layers to capture spatial features\n",
    "        x = keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(reshaped_input)\n",
    "        x = keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "        x = keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "        x = keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "        # Flatten spatial features for temporal processing\n",
    "        x = keras.layers.TimeDistributed(keras.layers.Flatten())(x)\n",
    "\n",
    "        # GRU layers for temporal modeling\n",
    "        x = keras.layers.GRU(16, return_sequences=True, recurrent_activation='sigmoid', reset_after=False)(\n",
    "            x, mask=mask_input\n",
    "        )\n",
    "        x = keras.layers.GRU(8, recurrent_activation='sigmoid', reset_after=True)(x)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = keras.layers.Dropout(0.4)(x)\n",
    "        x = keras.layers.Dense(224, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dense(56, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dense(14, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "\n",
    "        # Output layer\n",
    "        output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "        # Compile the model\n",
    "        rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "        rnn_model.compile(\n",
    "            loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        return rnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "        # Input layers\n",
    "        frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "        mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "        # Reshape input for convolution (adding a channel dimension)\n",
    "        reshaped_input = keras.layers.Reshape((MAX_SEQ_LENGTH, NUM_FEATURES, 1))(frame_features_input)\n",
    "\n",
    "        # Convolutional layers to capture spatial features\n",
    "        x = keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(reshaped_input)\n",
    "        x = keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "        x = keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "        x = keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "        # Flatten spatial features for temporal processing\n",
    "        x = keras.layers.TimeDistributed(keras.layers.Flatten())(x)\n",
    "\n",
    "        # GRU layers for temporal modeling\n",
    "        x = keras.layers.GRU(16, return_sequences=True, recurrent_activation='sigmoid', reset_after=False)(\n",
    "            x, mask=mask_input\n",
    "        )\n",
    "        x = keras.layers.GRU(8, recurrent_activation='sigmoid', reset_after=True)(x)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = keras.layers.Dropout(0.4)(x)\n",
    "        x = keras.layers.Dense(224, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dense(56, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dense(14, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "\n",
    "        # Output layer\n",
    "        output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "        # Compile the model\n",
    "        rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "        rnn_model.compile(\n",
    "            loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        return rnn_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
